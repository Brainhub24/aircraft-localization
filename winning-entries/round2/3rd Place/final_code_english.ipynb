{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packs and read round2 information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'round2/'\n",
    "training = pd.read_csv(path+'round2_competition.csv')#read data\n",
    "sensors = pd.read_csv(path+'round2_sensors.csv')#read sensors data\n",
    "validation = pd.read_csv(path+'Submission_db_NN_huigui.csv')#Read Submission file(as a submission model)\n",
    "testing = training.loc[training.id.isin(validation.id)]#Get test set（the aircraft that needs to location）\n",
    "training = training.dropna()#Delete information that has no location in the training set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coordinate transformation, WGS84 and ECEF\n",
    "from math import radians, cos, sin, asin, sqrt, pi\n",
    "\n",
    "# WGS84 to ECEF\n",
    "def llh2ecef(llh):#Latitude, longitude, altitude\n",
    "    \"\"\"Converts from WGS84 lat/lon/height to ellipsoid-earth ECEF\"\"\"\n",
    "    DTOR = pi / 180.0\n",
    "    # radians to degrees\n",
    "    RTOD = 180.0 / pi\n",
    "    WGS84_A = 6378137.0\n",
    "    WGS84_F = 1.0/298.257223563\n",
    "    WGS84_B = WGS84_A * (1 - WGS84_F)\n",
    "    WGS84_ECC_SQ = 1 - WGS84_B * WGS84_B / (WGS84_A * WGS84_A)\n",
    "    WGS84_ECC = sqrt(WGS84_ECC_SQ)\n",
    "    lat = llh[0] * DTOR\n",
    "    lng = llh[1] * DTOR\n",
    "    alt = llh[2]\n",
    "    slat = sin(lat)\n",
    "    slng = sin(lng)\n",
    "    clat = cos(lat)\n",
    "    clng = cos(lng)\n",
    "    d = sqrt(1 - (slat * slat * WGS84_ECC_SQ))\n",
    "    rn = WGS84_A / d\n",
    "    x = (rn + alt) * clat * clng\n",
    "    y = (rn + alt) * clat * slng\n",
    "    z = (rn * (1 - WGS84_ECC_SQ) + alt) * slat\n",
    "    x=round(x/1000,6)\n",
    "    y=round(y/1000,6)\n",
    "    z=round(z/1000,6)\n",
    "    return [x, y, z]#The unit of return is km\n",
    "\n",
    "# ECEF to WGS84\n",
    "def ecef2lla(aa):\n",
    "    \n",
    "    x,y,z = aa[0]*1000, aa[1]*1000, aa[2]*1000\n",
    "    # x, y and z are scalars or vectors in meters\n",
    "    x = np.array([x]).reshape(np.array([x]).shape[-1], 1)\n",
    "    y = np.array([y]).reshape(np.array([y]).shape[-1], 1)\n",
    "    z = np.array([z]).reshape(np.array([z]).shape[-1], 1)\n",
    "\n",
    "    a=6378137\n",
    "    a_sq=a**2\n",
    "    e = 8.181919084261345e-2\n",
    "    e_sq = 6.69437999014e-3\n",
    "\n",
    "    f = 1/298.257223563\n",
    "    b = a*(1-f)\n",
    "\n",
    "    # calculations:\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    ep_sq  = (a**2-b**2)/b**2\n",
    "    ee = (a**2-b**2)\n",
    "    f = (54*b**2)*(z**2)\n",
    "    g = r**2 + (1 - e_sq)*(z**2) - e_sq*ee*2\n",
    "    c = (e_sq**2)*f*r**2/(g**3)\n",
    "    s = (1 + c + np.sqrt(c**2 + 2*c))**(1/3.)\n",
    "    p = f/(3.*(g**2)*(s + (1./s) + 1)**2)\n",
    "    q = np.sqrt(1 + 2*p*e_sq**2)\n",
    "    r_0 = -(p*e_sq*r)/(1+q) + np.sqrt(0.5*(a**2)*(1+(1./q)) - p*(z**2)*(1-e_sq)/(q*(1+q)) - 0.5*p*(r**2))\n",
    "    u = np.sqrt((r - e_sq*r_0)**2 + z**2)\n",
    "    v = np.sqrt((r - e_sq*r_0)**2 + (1 - e_sq)*z**2)\n",
    "    z_0 = (b**2)*z/(a*v)\n",
    "    h = u*(1 - b**2/(a*v))\n",
    "    phi = np.arctan((z + ep_sq*z_0)/r)\n",
    "    lambd = np.arctan2(y, x)\n",
    "\n",
    "    \n",
    "    return [list(phi*180/np.pi)[0][0] , (lambd*180/np.pi)[0][0] , list(h)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removal of discrete points\n",
    "def three_sigma(dataset, n= 1):\n",
    "    mean = np.mean(dataset[:,1])\n",
    "    sigma = np.std(dataset[:,1])\n",
    " \n",
    "    remove_idx = np.where(abs(dataset[:,1] - mean) > n * sigma)\n",
    "    new_data = np.delete(dataset, remove_idx, 0)\n",
    " \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rearrange table\n",
    "def getmeasurements(df):\n",
    "    '''\n",
    "    input:df # such as round2_competition.csv\n",
    "    outputs:df[id, S, time, db ]\n",
    "    '''\n",
    "    measurements = df['measurements'].tolist()\n",
    "    ID = df['id'].tolist()\n",
    "    timeAtServer = df['timeAtServer'].tolist()\n",
    "    \n",
    "    measurements_list = []\n",
    "    \n",
    "    for index, measurement in enumerate(measurements):\n",
    "        # split the str df['measurements']\n",
    "        measurement = measurement[2:-2].split('],[')#Split according to the first dimension of the list\n",
    "        for value in measurement:\n",
    "            value = value.split(',')#Split according to the second dimension of the list\n",
    "            value = [ float(x) for x in value ]\n",
    "            measurements_list.append([ID[index], timeAtServer[index]]+value)\n",
    "    \n",
    "    dff = pd.DataFrame(measurements_list) #转换为dataframe\n",
    "    dff.rename(columns={0:'id', 1:'timeAtServer', 2:'serial', 3:'time', 4:'db'}, inplace = True)       \n",
    "    return dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get accurate sensor time difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate transformation of known data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coordinate transformation\n",
    "# Coordinate transformation of known points in the training data\n",
    "train_local_llh = training[['latitude','longitude','baroAltitude']].values\n",
    "train_local_xyz = []\n",
    "for i in train_local_llh:\n",
    "    train_local_xyz.append(llh2ecef(i))\n",
    "train_local_xyz = np.array(train_local_xyz).T\n",
    "training['x'] = train_local_xyz[0]\n",
    "training['y'] = train_local_xyz[1]\n",
    "training['z'] = train_local_xyz[2]\n",
    "\n",
    "# Coordinate transformation of sensor position\n",
    "S_local_llh = sensors[['latitude','longitude','height']].values\n",
    "S_local_xyz = []\n",
    "for i in S_local_llh:\n",
    "    S_local_xyz.append(llh2ecef(i))\n",
    "S_local_xyz = np.array(S_local_xyz).T\n",
    "sensors['S_x'] = S_local_xyz[0]\n",
    "sensors['S_y'] = S_local_xyz[1]\n",
    "sensors['S_z'] = S_local_xyz[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split information and make it into a usable DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and restructuring training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train data\n",
    "training_split = getmeasurements(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge the location of sensors to train data\n",
    "training_split = pd.merge(training_split, sensors, on=['serial'])\n",
    "training_split.rename(columns={'latitude':'S_latitude','longitude':'S_longitude','height':'S_height'}, inplace = True)\n",
    "del training_split['type']\n",
    "\n",
    "# merge the location of signal source to train data\n",
    "training_split = pd.merge(training_split, training[['id','latitude','longitude','geoAltitude','baroAltitude','aircraft','x','y','z']], on=['id'])\n",
    "training_split.sort_values(['id','serial'], inplace=True)\n",
    "training_split.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the theoretical flight time of each sensor received message for training_split\n",
    "dd = training_split[['S_x','S_y','S_z','x','y','z']].values.T\n",
    "tof = ((dd[0] - dd[3])**2 + (dd[1] - dd[4])**2 + (dd[2] - dd[5])**2)**0.5 * 10000/2.9965\n",
    "training_split['TOF'] = tof\n",
    "training_split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and restructuring predictable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_list = getmeasurements(testing)\n",
    "\n",
    "# merge the location of sensors to test_data\n",
    "test_data = pd.merge(testing_list, sensors, on=['serial'])\n",
    "test_data.rename(columns={'latitude':'S_latitude','longitude':'S_longitude','height':'S_height'}, inplace = True)\n",
    "del test_data['type']\n",
    "\n",
    "# merge the location of signal source to test data\n",
    "test_data = pd.merge(test_data, testing[['id','baroAltitude','aircraft']], on=['id'])\n",
    "test_data.sort_values(['id','serial'], inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## According to the sensor group, calculate the time difference between two sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load sensor pair information\n",
    "pair_set = np.load('pair_set.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "test_without_shift = pd.DataFrame({'id':[],'time':[], 'S1':[], 'S2':[], 'tdoa':[]})\n",
    "\n",
    "# The data is traversed according to the sensor\n",
    "for pair in pair_set:\n",
    "    i+=1\n",
    "    try:\n",
    "        training_split_2true = training_split[['id','serial','time','TOF','aircraft']].loc[(training_split.serial.isin([pair[0],pair[1]]))]\n",
    "        df_out = training_split_2true.set_index(['id',training_split_2true.groupby('id').cumcount()+1]).unstack().sort_index(level=1, axis=1)\n",
    "        df_out.columns = df_out.columns.map('{0[0]}_{0[1]}'.format)\n",
    "        df_out.dropna(inplace = True)\n",
    "        df_out.drop(['aircraft_2'], axis=1,inplace = True)\n",
    "        df_out.reset_index()\n",
    "\n",
    "        test_2true = test_data[['id','serial','time','aircraft']].loc[(test_data.serial.isin([pair[0],pair[1]]))]\n",
    "        test_out = test_2true.set_index(['id',test_2true.groupby('id').cumcount()+1]).unstack().sort_index(level=1, axis=1)\n",
    "        test_out.columns = test_out.columns.map('{0[0]}_{0[1]}'.format)\n",
    "        test_out.dropna(inplace = True)\n",
    "        test_out.drop(['aircraft_2'], axis=1,inplace = True)\n",
    "        test_out.reset_index()\n",
    "\n",
    "\n",
    "        xy = np.array([df_out.time_1,df_out.time_1.values - df_out.time_2.values -(df_out.TOF_1.values - df_out.TOF_2.values)]).T\n",
    "        test_xy = np.array([test_out.time_1,test_out.time_1.values - test_out.time_2.values, test_out.index]).T\n",
    "\n",
    "        # For the first time, linear fitting is used to train the data clock error,\n",
    "        # The error function is subtracted from the test data\n",
    "        xy = three_sigma(xy,n = 1)\n",
    "        z1 = np.polyfit(xy[:,0],xy[:,1], 1)  # Linear fitting\n",
    "        p1 = np.poly1d(z1)\n",
    "        xy[:,1] = xy[:,1] - p1(xy[:,0])\n",
    "        test_xy[:,1] = test_xy[:,1] - p1(test_xy[:,0])\n",
    "\n",
    "        test_xy = test_xy[np.where(xy[0,0] <test_xy[:,0])]\n",
    "        test_xy = test_xy[np.where(xy[-1,0]>test_xy[:,0])]\n",
    "        \n",
    "        # The second time, the clock error of the training data was fitted with the 7-degree curve\n",
    "        # The error function is subtracted from the test data\n",
    "        xy = three_sigma(xy,n = 3)\n",
    "        z1 = np.polyfit(xy[:,0],xy[:,1], 7)  #7th degree polynomial fitting\n",
    "        p1 = np.poly1d(z1)\n",
    "        z2 = p1(xy[:,0])\n",
    "        xy[:,1] = xy[:,1] - z2\n",
    "        test_xy[:,1] = test_xy[:,1] - p1(test_xy[:,0])\n",
    "\n",
    "        # The third time, the clock error of the training data was fitted with the 20-degree curve\n",
    "        # The error function is subtracted from the test data\n",
    "        xy = three_sigma(xy,n = 3)\n",
    "        z1 = np.polyfit(xy[:,0],xy[:,1], 20)  #20th degree polynomial fitting\n",
    "        p1 = np.poly1d(z1)\n",
    "        z2 = p1(xy[:,0])\n",
    "        xy[:,1] = xy[:,1] - z2\n",
    "        test_xy[:,1] = test_xy[:,1] - p1(test_xy[:,0])\n",
    "        \n",
    "        # The forth time, the clock error of the training data was fitted with the 20-degree curve\n",
    "        # The error function is subtracted from the test data\n",
    "        xy = three_sigma(xy,n = 3)\n",
    "        test_xy = three_sigma(test_xy,n = 3)\n",
    "        z1 = np.polyfit(xy[:,0],xy[:,1], 20)  #20th degree polynomial fitting\n",
    "        p1 = np.poly1d(z1)\n",
    "        z2 = p1(xy[:,0])\n",
    "        xy[:,1] = xy[:,1] - z2\n",
    "        test_xy[:,1] = test_xy[:,1] - p1(test_xy[:,0])\n",
    "        \n",
    "        # If the residual clock error is less than 1000ns, no further operation is carried out\n",
    "        # The time difference can be directly used\n",
    "        if (max(xy[:,1]) - min(xy[:,1]))<1000:\n",
    "            test_xy = three_sigma(test_xy,n = 3)\n",
    "            \n",
    "        # If the residual clock error is greater than 1000ns and less than 5000ns, \n",
    "        # interpolation method is used for further error processing\n",
    "        elif 1000<(max(xy[:,1]) - min(xy[:,1]))<50000:\n",
    "            test_xy = test_xy[np.where(xy[0,0] <test_xy[:,0])]\n",
    "            test_xy = test_xy[np.where(xy[-1,0]>test_xy[:,0])]\n",
    "            test_xy = three_sigma(test_xy,n = 3)\n",
    "            f=interpolate.interp1d(xy[:,0],xy[:,1],kind='slinear')\n",
    "            xy[:,1]=xy[:,1] - f(xy[:,0])\n",
    "            test_xy[:,1]=test_xy[:,1] - f(test_xy[:,0])\n",
    "            test_xy = three_sigma(test_xy,n = 7)\n",
    "            \n",
    "        # If the remaining clock error is greater than 5000ns, this part of the data is discarded\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # merge data and coexist as DataFrame\n",
    "        idd = test_xy[:,2]\n",
    "        S1 = [pair[0]]*len(idd)\n",
    "        S2 = [pair[1]]*len(idd)\n",
    "        time = test_xy[:,0]\n",
    "        tdoa = test_xy[:,1]\n",
    "        test_without_shift = pd.concat([test_without_shift, pd.DataFrame({'id':idd,'time':time, 'S1':S1, 'S2':S2, 'tdoa':tdoa})])\n",
    "\n",
    "        print(i,pair)\n",
    "    except:\n",
    "        print(i, pair, 'error')\n",
    "test_without_shift\n",
    "# 5080185 rows × 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort and save the test data minus clock errors\n",
    "test_without_shift.sort_values(['id','S1','S2'], inplace=True)\n",
    "test_without_shift.to_csv('test_without_shift.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'result_new.csv' was obtained by TDOA positioning with the sensor time difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TDOA code is in a separate file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The location results were fitted according to the trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv('round2/round2_competition.csv')#read data\n",
    "result_new = pd.read_csv('round2/result_new.csv')\n",
    "result =  pd.read_csv('round2/Submission_db_NN_huigui.csv')\n",
    "result = pd.merge(result, result_new[['id','lattitude','longtitude','height']], on=['id'], how = 'left')\n",
    "result = pd.merge(result, training[['id','aircraft','timeAtServer' ]], on=['id'], how = 'left')\n",
    "# result.loc[(~result.lattitude.isna()), 'latitude'] = result.loc[(~result.lattitude.isna())]['lattitude']\n",
    "# result.loc[(~result.lattitude.isna()), 'longitude'] = result.loc[(~result.lattitude.isna())]['longtitude']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate distances using latitude and longitude coordinates\n",
    "from math import radians, cos, sin, asin, sqrt, pi, atan, pow\n",
    "def geodistance(start, destination):\n",
    "    lng1, lat1,lng2, lat2 = start[0],start[1],destination[0],destination[1]\n",
    "    # Latitude and longitude are converted into radians\n",
    "    lng1, lat1, lng2, lat2 = map(radians, [float(lng1), float(lat1), float(lng2), float(lat2)])\n",
    "    dlon = lng2 - lng1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    distance = 2 * asin(sqrt(a)) * 6371   # The average radius of the earth,6371km\n",
    "    distance = round(distance, 3)\n",
    "    return distance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pandas import Series\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Use latitude and longitude to remove outliers\n",
    "def three_sigma(dataset, n = 1):\n",
    "    mean = np.mean(dataset[:,0])\n",
    "    sigma = np.std(dataset[:,0])\n",
    "    remove_idx = np.where(abs(dataset[:,0] - mean) > n * sigma)\n",
    "    dataset = np.delete(dataset, remove_idx, 0)\n",
    "    \n",
    "    mean = np.mean(dataset[:,1])\n",
    "    sigma = np.std(dataset[:,1])\n",
    "    remove_idx = np.where(abs(dataset[:,1] - mean) > n * sigma)\n",
    "    new_data = np.delete(dataset, remove_idx, 0)\n",
    " \n",
    "    return new_data\n",
    "\n",
    "location = pd.DataFrame({'id':[], 'latitude':[], 'longitude':[]})\n",
    "i = 0 # count\n",
    "for aircraft, group in result.groupby('aircraft'):\n",
    "    i+=1\n",
    "    # This aircraft has poor positioning results and is not to be used\n",
    "    if aircraft == 2820:\n",
    "        continue\n",
    "    try:\n",
    "        groupdata = np.array([group.lattitude_x.tolist(),group.longtitude_x.tolist(),\n",
    "                              group.timeAtServer.tolist(),group.height.tolist(),group.geoAltitude.tolist()]).T\n",
    "        # Remove the NAN\n",
    "        remove_idx = np.where(np.isnan(groupdata)==True)\n",
    "        groupdata = np.delete(groupdata, remove_idx, 0)\n",
    "        # Remove the value with large height error\n",
    "        remove_idx = np.where(np.abs(groupdata[:,4]-groupdata[:,3])>30000)\n",
    "        groupdata = np.delete(groupdata, remove_idx, 0)\n",
    "        # Remove the outliers\n",
    "        groupdata = three_sigma(groupdata, n = 3)\n",
    "        # Get the data you need\n",
    "        lat,lon,time = groupdata[:,0], groupdata[:,1], groupdata[:,2]\n",
    "\n",
    "        # Cubic curves were fitted with time to latitude and longitude respectively\n",
    "        z1 = np.polyfit(time, lat, 3)  #3th degree polynomial fitting\n",
    "        p1 = np.poly1d(z1)\n",
    "        z2 = np.polyfit(time, lon, 3)  #3th degree polynomial fitting\n",
    "        p2 = np.poly1d(z2)\n",
    "\n",
    "        # Subtract the fitting curve and straighten out the trajectory\n",
    "        groupdata[:,0] = groupdata[:,0] - p1(groupdata[:,2])\n",
    "        groupdata[:,1] = groupdata[:,1] - p2(groupdata[:,2])\n",
    "\n",
    "        # Remove the error points are removed at aircraft speed\n",
    "        \n",
    "        d2 = np.abs(np.array([geodistance(groupdata[i+1,0:2], groupdata[i,0:2])/ (groupdata[i+1,2]-groupdata[i,2])\n",
    "                              for i in range(0, groupdata.shape[0]-1)]))\n",
    "        groupdata = groupdata[np.where(d2 < 0.25)[0]+1]\n",
    "\n",
    "        # Remove the error points according to the turn speed of the aircraft\n",
    "        k = 1\n",
    "        d2 = np.abs(np.array([(geodistance(groupdata[i+k,0:2], groupdata[i-k,0:2]) -\n",
    "                              geodistance(groupdata[i+k,0:2], groupdata[i,0:2]) -\n",
    "                              geodistance(groupdata[i,0:2], groupdata[i-k,0:2]))/\n",
    "                               (groupdata[i+k,2]-groupdata[k,2])\n",
    "                              for i in range(k, groupdata.shape[0]-k)]))\n",
    "        groupdata = groupdata[np.where(d2 < 0.02)[0]+k]\n",
    "\n",
    "        # Add the fitting curve and restore the trajectory\n",
    "        groupdata[:,0] = groupdata[:,0] + p1(groupdata[:,2])\n",
    "        groupdata[:,1] = groupdata[:,1] + p2(groupdata[:,2])\n",
    "\n",
    "        # To locate the data in accordance with a certain range\n",
    "        labeldata = np.array([group.id.tolist(),group.timeAtServer.tolist()]).T\n",
    "        labeldata = labeldata[np.where(min(groupdata[:,2]) <labeldata[:,1])]\n",
    "        labeldata = labeldata[np.where(max(groupdata[:,2])>labeldata[:,1])]\n",
    "        idd = labeldata[:,0]\n",
    "        timeAtServer = labeldata[:,1]\n",
    "\n",
    "        # If the fitting point is less than 50, the trajectory is abandoned\n",
    "        if len(groupdata[:,2])<50:\n",
    "            plt.close()\n",
    "            continue\n",
    "\n",
    "        # Linear interpolation is performed using the screened data\n",
    "        f=interpolate.interp1d(groupdata[:,2],groupdata[:,0],kind='slinear')\n",
    "        new_lat=f(timeAtServer)\n",
    "        f=interpolate.interp1d(groupdata[:,2],groupdata[:,1],kind='slinear')\n",
    "        new_lon=f(timeAtServer)\n",
    "\n",
    "        # The interpolated trajectories were fitted by piecewise curves\n",
    "        # Define the parameters of piecewise curve fitting\n",
    "        if aircraft not in [451,546,601,824,1008,1020,1633,1913,2758,2841 ]:\n",
    "            num_divide = 120 # Sectional interval size\n",
    "            cishu = 3\n",
    "        elif aircraft in [1020,1913,2758,2841]:\n",
    "            num_divide = 10000\n",
    "            cishu = 20\n",
    "        else:\n",
    "            num_divide = 300\n",
    "            cishu = 3\n",
    "            \n",
    "        # Piecewise fitting\n",
    "        num_points = len(idd) # num of points\n",
    "        num_section = num_points // num_divide\n",
    "        print(num_section)\n",
    "        start = 0\n",
    "        lat_section = pd.DataFrame({'id':[], 'latitude':[], 'longitude':[]})\n",
    "        new_lat2 = new_lon2 = []\n",
    "        for section in range(num_section+1):\n",
    "            end = start+num_divide\n",
    "            if end > num_points:\n",
    "                end = num_points\n",
    "\n",
    "            z2 = np.polyfit(timeAtServer[start:end],new_lat[start:end],cishu)\n",
    "            p2 = np.poly1d(z2)\n",
    "\n",
    "            new_lat2 = new_lat2 +p2(timeAtServer[start:end]).tolist()\n",
    "            z2 = np.polyfit(timeAtServer[start:end],new_lon[start:end],cishu)\n",
    "            p2 = np.poly1d(z2)\n",
    "            new_lon2 += p2(timeAtServer[start:end]).tolist()\n",
    "            start += num_divide\n",
    "            \n",
    "        # Merge and save data\n",
    "        location = pd.DataFrame({'id':idd, 'latitude':new_lat2, 'longitude':new_lon2})\n",
    "        locations = pd.concat([locations,location])\n",
    "        print(aircraft,len(new_lon))\n",
    "        \n",
    "    except:\n",
    "        plt.close()\n",
    "        print('error')\n",
    "locations.sort_values('id', inplace = True)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the final result\n",
    "Submission =  pd.read_csv('round2/Submission_db_NN_huigui.csv')\n",
    "Submission = pd.merge(Submission, locations, on=['id'], how = 'left')\n",
    "Submission = Submission[['id','latitude_y','longitude_y','geoAltitude']]\n",
    "Submission.rename(columns={'latitude_y':'latitude','longitude_y':'longitude'}, inplace = True)\n",
    "Submission.to_csv('Submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
