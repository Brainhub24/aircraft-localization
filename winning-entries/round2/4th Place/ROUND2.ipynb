{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSN Competition round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /cluster/home/figu/OSN_competition/round2/delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import plotly.graph_objects as go\n",
    "import geopy\n",
    "from plotly.subplots import make_subplots\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from itertools import combinations\n",
    "import geopy.distance\n",
    "import pickle\n",
    "from scipy.optimize import fsolve, root\n",
    "from scipy.stats import iqr\n",
    "import warnings\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('round2_competition.csv')\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_ids = []\n",
    "# ids2remove = []\n",
    "# thr = 1e4\n",
    "# for ac_id in tqdm(list(training.aircraft.unique())):\n",
    "#     tac = training.loc[training.aircraft==ac_id]\n",
    "#     d = np.sqrt(np.diff(tac.latitude)**2+np.diff(tac.longitude)**2)*111000/np.diff(tac.timeAtServer).tolist()\n",
    "#     if np.any(d>thr):\n",
    "#         outliers_ids = tac.iloc[np.where(d>thr)].id.values.tolist()\n",
    "#         ids2remove += outliers_ids\n",
    "#         traj_out = tac.loc[tac.id.isin(outliers_ids)]\n",
    "#         plt.figure()\n",
    "#         plt.scatter(tac.longitude, tac.latitude)\n",
    "#         plt.scatter(traj_out.longitude, traj_out.latitude, color='red')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('round2_sample_empty.csv')\n",
    "print(len(sample))\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = pd.read_csv('round2_sensors.csv')\n",
    "sensors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sensors = sensors.loc[sensors.good].serial.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand columns / rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_measurements(df):\n",
    "    # Function to expand the json coumn called measurements\n",
    "    dfs = []\n",
    "    def json_to_df(row, json_col):\n",
    "        json_df = pd.read_json(row[json_col])\n",
    "        dfs.append(json_df.assign(**row.drop(json_col)))\n",
    "    df.apply(json_to_df, axis=1, json_col='measurements')   \n",
    "    return pd.concat(dfs).reset_index()\n",
    "\n",
    "def expand_measurements_para(df):\n",
    "    # Here we used multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    chunk_size = int((len(df)//num_processes)+1)\n",
    "    chunks = [df.iloc[i:i + chunk_size]for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(expand_measurements, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_training = pd.read_pickle('df_competition.pkl')\n",
    "except:\n",
    "    df_training = expand_measurements_para(training)\n",
    "    df_training.to_pickle('df_competition.pkl')\n",
    "df_training = df_training.rename(columns={0: 'sensor', 1:'timestamp', 2:'power'})\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_training.id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = df_training.loc[df_training.latitude.isnull()]\n",
    "df_training = df_training.loc[~df_training.latitude.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise and triplet wise\n",
    "Here we extract data to have pairwise / triplet wise information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs_dt_n_power(df):\n",
    "    rows_list = []\n",
    "    grouped = df.groupby('id')\n",
    "    for ids, group in tqdm(grouped, total=len(grouped)):\n",
    "        sensor_pairs = [sorted(t) for t in list(combinations(group.sensor, 2))]\n",
    "        for pair in sensor_pairs:\n",
    "            dt_obs = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[1]].timestamp.values[0]\n",
    "            p0 = group.loc[group.sensor==pair[0]].power.values[0]\n",
    "            p1 = group.loc[group.sensor==pair[1]].power.values[0]\n",
    "            tAtServer = group.loc[group.sensor==pair[1]].timeAtServer.values[0]\n",
    "            lat = group.latitude.values[0]\n",
    "            lon = group.longitude.values[0]\n",
    "            baro = group.baroAltitude.values[0]\n",
    "            geo =  group.geoAltitude.values[0]\n",
    "            rows_list.append({'id':ids, 's0':pair[0], 's1':pair[1], 'dt_obs':dt_obs,\n",
    "                              'p0': p0, 'p1': p1, 'timeAtServer': tAtServer,\n",
    "                              'latitude': lat, 'longitude': lon, 'baroAltitude': baro, 'geoAltitude': geo})\n",
    "    print('Job done')\n",
    "    return pd.DataFrame(rows_list)\n",
    "\n",
    "def get_pairs_dt_n_power_para(df):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    grouped = df.groupby('id')\n",
    "    list_groups = [g[1] for g in list(grouped)]\n",
    "    chunk_size = int((len(list_groups)//num_processes)+1)                      \n",
    "    chunks = [pd.concat(list_groups[i:i+chunk_size]) for i in range(0, len(list_groups), chunk_size)]\n",
    "    print('Start // computation')\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_pairs_dt_n_power, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)\n",
    "\n",
    "def get_triplet_dt_n_power(list_ids):\n",
    "    rows_list = []\n",
    "    for ids in tqdm(list_ids):\n",
    "        group = df2.loc[df2.id==ids]\n",
    "        if len(group)<3:\n",
    "            continue\n",
    "        sensor_pairs = [sorted(t) for t in list(combinations(group.sensor, 3))]\n",
    "        for pair in sensor_pairs:\n",
    "            dt_01 = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[1]].timestamp.values[0]\n",
    "            dt_02 = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[2]].timestamp.values[0]\n",
    "            dt_12 = group.loc[group.sensor==pair[1]].timestamp.values[0]-group.loc[group.sensor==pair[2]].timestamp.values[0]\n",
    "            p0 = group.loc[group.sensor==pair[0]].power.values[0]\n",
    "            p1 = group.loc[group.sensor==pair[1]].power.values[0]\n",
    "            p2 = group.loc[group.sensor==pair[2]].power.values[0]\n",
    "            tAtServer = group.loc[group.sensor==pair[1]].timeAtServer.values[0]\n",
    "            lat = group.latitude.values[0]\n",
    "            lon = group.longitude.values[0]\n",
    "            baro = group.baroAltitude.values[0]\n",
    "            geo =  group.geoAltitude.values[0]\n",
    "            rows_list.append({'id':ids, 's0':pair[0], 's1':pair[1], 's2':pair[2],\n",
    "                              'dt01':dt_01, 'dt02':dt_02, 'dt12':dt_12,\n",
    "                              'p0': p0, 'p1': p1, 'p2':p2, 'timeAtServer': tAtServer,\n",
    "                              'latitude': lat, 'longitude': lon, 'baroAltitude': baro, 'geoAltitude': geo})\n",
    "    return pd.DataFrame(rows_list)\n",
    "\n",
    "def get_triplet_dt_n_power_para(df):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    global df2\n",
    "    df2 = df.copy()\n",
    "    num_processes = mp.cpu_count()\n",
    "    list_ids = df.id.unique()\n",
    "    chunk_size = int((len(list_ids)//num_processes)+1)                      \n",
    "    chunks = [list_ids[i:i + chunk_size]for i in range(0, len(list_ids), chunk_size)]\n",
    "    print('Start // computation')\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_triplet_dt_n_power, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)\n",
    "\n",
    "\n",
    "def get_triplet_dt_n_power2(list_ids):\n",
    "    rows_list = []\n",
    "    for ids in tqdm(list_ids):\n",
    "        group = dico_groups2[ids]\n",
    "        if len(group)<3:\n",
    "            continue\n",
    "        sensor_pairs = [sorted(t) for t in list(combinations(group.sensor, 3))]\n",
    "        for pair in sensor_pairs:\n",
    "            dt_01 = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[1]].timestamp.values[0]\n",
    "            dt_02 = group.loc[group.sensor==pair[0]].timestamp.values[0]-group.loc[group.sensor==pair[2]].timestamp.values[0]\n",
    "            dt_12 = group.loc[group.sensor==pair[1]].timestamp.values[0]-group.loc[group.sensor==pair[2]].timestamp.values[0]\n",
    "            p0 = group.loc[group.sensor==pair[0]].power.values[0]\n",
    "            p1 = group.loc[group.sensor==pair[1]].power.values[0]\n",
    "            p2 = group.loc[group.sensor==pair[2]].power.values[0]\n",
    "            tAtServer = group.loc[group.sensor==pair[1]].timeAtServer.values[0]\n",
    "            lat = group.latitude.values[0]\n",
    "            lon = group.longitude.values[0]\n",
    "            baro = group.baroAltitude.values[0]\n",
    "            geo =  group.geoAltitude.values[0]\n",
    "            rows_list.append({'id':ids, 's0':pair[0], 's1':pair[1], 's2':pair[2],\n",
    "                              'dt01':dt_01, 'dt02':dt_02, 'dt12':dt_12,\n",
    "                              'p0': p0, 'p1': p1, 'p2':p2, 'timeAtServer': tAtServer,\n",
    "                              'latitude': lat, 'longitude': lon, 'baroAltitude': baro, 'geoAltitude': geo})\n",
    "    return pd.DataFrame(rows_list)\n",
    "\n",
    "\n",
    "def get_triplet_dt_n_power_para2(dico_groups):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    global dico_groups2\n",
    "    dico_groups2 = dico_groups.copy()\n",
    "    num_processes = mp.cpu_count()\n",
    "    list_ids = list(dico_groups.keys())\n",
    "    chunk_size = int((len(list_ids)//num_processes)+1)                      \n",
    "    chunks = [list_ids[i:i + chunk_size]for i in range(0, len(list_ids), chunk_size)]\n",
    "    print('Start // computation')\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_triplet_dt_n_power2, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise information on the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take several hours to run (> 2hours on my machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train_pairs = pd.read_pickle('X_train_pairs.pkl')\n",
    "except:\n",
    "    X_train_pairs = get_pairs_dt_n_power_para(df_training)\n",
    "    X_train_pairs.to_pickle('X_train_pairs.pkl')\n",
    "X_train_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tripletwise data for the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train_triplet = pd.read_pickle('X_comp_trip.pkl')\n",
    "except:\n",
    "    grouped = df_training.groupby('id')\n",
    "    dico_groups = {ids: dft for ids, dft in tqdm(grouped)}\n",
    "    dico_groups_sub = {ids:dico_groups[ids] for cpt, ids in enumerate(dico_groups) if cpt < 200000}\n",
    "    X_train_triplet = get_triplet_dt_n_power_para2(dico_groups_sub)\n",
    "    X_train_triplet.to_pickle('X_comp_trip.pkl')\n",
    "X_train_triplet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_test_triplet = pd.read_pickle('X_test_trip.pkl')\n",
    "except:\n",
    "    grouped = df_testing.groupby('id')\n",
    "    dico_groups = {ids: dft for ids, dft in tqdm(grouped)}\n",
    "    dico_groups_sub = {ids:dico_groups[ids] for cpt, ids in enumerate(dico_groups)}\n",
    "    X_test_triplet = get_triplet_dt_n_power_para2(dico_groups_sub)\n",
    "    X_test_triplet.to_pickle('X_test_trip.pkl')\n",
    "X_test_triplet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Barometric altitude estimation using LGBM\n",
    "## Put in in format for lgbm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_triplet.copy()\n",
    "X_test = X_test_triplet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_sensors = {ids+1: tuple(sensors.iloc[ids][['latitude', 'longitude','height']].values) for ids in range(len(sensors))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['s0_lla'] = [dico_sensors[x] for x in X_train.s0.values]\n",
    "X_train['s1_lla'] = [dico_sensors[x] for x in X_train.s1.values]\n",
    "X_train['s2_lla'] = [dico_sensors[x] for x in X_train.s2.values]\n",
    "X_train[['s0_lat', 's0_lon', 's0_alt']] = pd.DataFrame(X_train['s0_lla'].tolist(), index=X_train.index)\n",
    "X_train[['s1_lat', 's1_lon', 's1_alt']] = pd.DataFrame(X_train['s1_lla'].tolist(), index=X_train.index)\n",
    "X_train[['s2_lat', 's2_lon', 's2_alt']] = pd.DataFrame(X_train['s2_lla'].tolist(), index=X_train.index)\n",
    "\n",
    "X_train.id = X_train.id.astype(np.uint32)\n",
    "X_train[['p0', 'p1', 'p2']] = X_train[['p0', 'p1', 'p2']].astype(np.uint8)\n",
    "X_train[['latitude', 'longitude', 'baroAltitude', 'geoAltitude']] = X_train[['latitude', 'longitude', 'baroAltitude', 'geoAltitude']].astype(np.float32)\n",
    "\n",
    "X_train[\"s0\"] = X_train[\"s0\"].astype('category')\n",
    "X_train[\"s1\"] = X_train[\"s1\"].astype('category')\n",
    "X_train[\"s2\"] = X_train[\"s2\"].astype('category')\n",
    "\n",
    "X_train['s0s1s2'] = list(zip(X_train['s0'], X_train['s1'], X_train['s2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['s0_lla'] = [dico_sensors[x] for x in X_test.s0.values]\n",
    "X_test['s1_lla'] = [dico_sensors[x] for x in X_test.s1.values]\n",
    "X_test['s2_lla'] = [dico_sensors[x] for x in X_test.s2.values]\n",
    "X_test[['s0_lat', 's0_lon', 's0_alt']] = pd.DataFrame(X_test['s0_lla'].tolist(), index=X_test.index)\n",
    "X_test[['s1_lat', 's1_lon', 's1_alt']] = pd.DataFrame(X_test['s1_lla'].tolist(), index=X_test.index)\n",
    "X_test[['s2_lat', 's2_lon', 's2_alt']] = pd.DataFrame(X_test['s2_lla'].tolist(), index=X_test.index)\n",
    "\n",
    "X_test.id = X_test.id.astype(np.uint32)\n",
    "X_test[['p0', 'p1', 'p2']] = X_test[['p0', 'p1', 'p2']].astype(np.uint8)\n",
    "X_test[['latitude', 'longitude', 'baroAltitude', 'geoAltitude']] = X_test[['latitude', 'longitude', 'baroAltitude', 'geoAltitude']].astype(np.float32)\n",
    "\n",
    "\n",
    "X_test[\"s0\"] = X_test[\"s0\"].astype('category')\n",
    "X_test[\"s1\"] = X_test[\"s1\"].astype('category')\n",
    "X_test[\"s2\"] = X_test[\"s2\"].astype('category')\n",
    "\n",
    "X_test['s0s1s2'] = list(zip(X_test['s0'], X_test['s1'], X_test['s2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_cols = ['p0', 'p1', 'p2', 'timeAtServer', 'baroAltitude',\n",
    "              's0_lat', 's0_lon', 's0_alt', 's1_lat', 's1_lon', 's1_alt', 's2_lat', 's2_lon', 's2_alt']\n",
    "\n",
    "y_col = ['geoAltitude']\n",
    "X_training = X_train[train_cols]\n",
    "Y = X_train[y_col]\n",
    "\n",
    "# We will train 2 models each on one half of the data and sum it up at the end\n",
    "X_training, x_valid, y_train, y_valid = train_test_split(X_training, Y, test_size=0.3, random_state=42)\n",
    "d_train = lgb.Dataset(X_training, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "watchlist = [d_valid]\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'num_leaves': 20,\n",
    "    'learning_rate': 0.4,\n",
    "    'verbose': 0, \n",
    "    'early_stopping_round': 5,\n",
    "    }\n",
    "n_estimators = 50\n",
    "\n",
    "\n",
    "model_alt = lgb.train(params, d_train, n_estimators, watchlist, verbose_eval=1,\n",
    "                       learning_rates=lambda iter: 0.01+0.8 * (0.99995 ** iter),\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model predicitons on a random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "samp = X_train.sample(2000000)\n",
    "plt.hist(model_alt.predict(samp[train_cols])-samp.geoAltitude, 100)\n",
    "plt.hist(samp.baroAltitude-samp.geoAltitude, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.geoAltitude = model_alt.predict(X_test[train_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Time Synchronizations\n",
    "For each pair of sensor, we will try to estimate the function (a polynome) that describe the sensors tdoa error:\n",
    "$tdoa_{error} = tdoa_{observed}-tdoa_{computed}$. And then we try to find a function for each pair: $error(s0, s1, t1) = a0 + a1*t +... an*t^n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute theoretical TDOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calc_dt(r):\n",
    "    try:\n",
    "        d0 = geopy.distance.distance((r.s0_lat, r.s0_lon), (r.latitude, r.longitude)).m\n",
    "        d0 = np.sqrt(d0**2 + (r.s0_alt-r.geoAltitude)**2)\n",
    "        d1 = geopy.distance.distance((r.s1_lat, r.s1_lon), (r.latitude, r.longitude)).m\n",
    "        d1 = np.sqrt(d1**2 + (r.s1_alt-r.geoAltitude)**2)\n",
    "        dt01_calc = (d0 - d1)/0.2995\n",
    "        return dt01_calc\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def get_calc_dt_df(df):\n",
    "    df['dt_calc'] = df.apply(lambda r: get_calc_dt(r), 1)\n",
    "    return df\n",
    "\n",
    "def get_calc_dt_df_para(df):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    num_processes = mp.cpu_count()\n",
    "    chunk_size = int((len(df)//num_processes)+1)                      \n",
    "    chunks = [df.iloc[i:i + chunk_size]for i in range(0, len(df), chunk_size)]\n",
    "    print('Start // computation')\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_calc_dt_df, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)\n",
    "\n",
    "try:\n",
    "    X_train_pairs = pd.read_pickle('X_train_pairs_dt_calc.pkl')\n",
    "except:\n",
    "    X_train_pairs['s0_lla'] = [dico_sensors[x] for x in X_train_pairs.s0.values]\n",
    "    X_train_pairs['s1_lla'] = [dico_sensors[x] for x in X_train_pairs.s1.values]\n",
    "    X_train_pairs[['s0_lat', 's0_lon', 's0_alt']] = pd.DataFrame(X_train_pairs['s0_lla'].tolist(), index=X_train_pairs.index)\n",
    "    X_train_pairs[['s1_lat', 's1_lon', 's1_alt']] = pd.DataFrame(X_train_pairs['s1_lla'].tolist(), index=X_train_pairs.index)\n",
    "    X_train_pairs = get_calc_dt_df_para(X_train_pairs)\n",
    "    X_train_pairs.to_pickle('X_train_pairs_dt_calc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We retrieve all the pairs that are in the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = (X_test[['s0', 's1']].values.tolist() +\n",
    "         X_test[['s0', 's2']].values.tolist() + \n",
    "         X_test[['s1', 's2']].values.tolist())\n",
    "pairs = set([tuple(p) for p in pairs])\n",
    "print(len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pairs = X_train_pairs.dropna(subset=['dt_calc'])\n",
    "print(len(X_train_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For all pairs we compute the polynom that improve the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(points, thresh=3):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "def get_dico_corrections(pairs):\n",
    "    score_rng = [15, 85]\n",
    "    dico_poly_correction = {}\n",
    "    pairs = list(pairs)\n",
    "    for id0, id1 in tqdm(pairs):\n",
    "        t_test = X_test.loc[((X_test.s0==id0) & (X_test.s1==id1))\n",
    "                           | ((X_test.s1==id0) & (X_test.s2==id1) )\n",
    "                           | ((X_test.s0==id0) & (X_test.s2==id1))].timeAtServer\n",
    "        # First we retrieve the theoretical tdoa\n",
    "        sub_pair = X_train_pairs.loc[(X_train_pairs.s0==id0) & (X_train_pairs.s1==id1)]\n",
    "        if len(sub_pair)<2:\n",
    "            continue\n",
    "        sub_pair['err'] = sub_pair['dt_obs']-sub_pair['dt_calc']\n",
    "        # then we fist a 1st order to eliminate outliers\n",
    "        poly = np.poly1d(np.polyfit(sub_pair['timeAtServer'], sub_pair['err'], 1))\n",
    "        sub_pair['diff'] = sub_pair['err']-poly(sub_pair.timeAtServer)\n",
    "        sub_pair = sub_pair.loc[~is_outlier(sub_pair['diff'], 3.5)]\n",
    "        #############\n",
    "        poly = np.poly1d(np.polyfit(sub_pair['timeAtServer'], sub_pair['err'], 1))\n",
    "        sub_pair['diff'] = sub_pair['err']-poly(sub_pair.timeAtServer)\n",
    "        sub_pair = sub_pair.loc[~is_outlier(sub_pair['diff'], 3.5)]\n",
    "        ######################\n",
    "\n",
    "        # then we refit a 1st  and a 30st order on the filtered version\n",
    "        poly_best = poly1 = np.poly1d(np.polyfit(sub_pair['timeAtServer'], sub_pair['err'], 1))\n",
    "        diff_best = sub_pair['err']-poly_best(sub_pair.timeAtServer)\n",
    "        smin = iqr(diff_best, rng=score_rng)\n",
    "        lsp = len(sub_pair)\n",
    "        if lsp > 10:\n",
    "            for i in range(1, np.min([int(lsp/10), 30])):\n",
    "                poly = np.poly1d(np.polyfit(sub_pair['timeAtServer'], sub_pair['err'], i))\n",
    "                diff = sub_pair['err']-poly(sub_pair.timeAtServer)\n",
    "                score = iqr(diff, rng=score_rng)\n",
    "                if score < smin:\n",
    "                    smin = score\n",
    "                    poly_best = poly\n",
    "        \n",
    "        diff1 = sub_pair['err']-poly1(sub_pair.timeAtServer)\n",
    "        diff_best = sub_pair['err']-poly_best(sub_pair.timeAtServer)\n",
    "        dico_poly_correction[id0, id1] = (poly_best,\n",
    "                                          iqr(diff_best, rng=score_rng),\n",
    "                                          sub_pair['timeAtServer'].values,\n",
    "                                          poly1,\n",
    "                                          iqr(diff1, rng=score_rng))\n",
    "    return dico_poly_correction\n",
    "\n",
    "try:\n",
    "    with open('dico_poly_correction.pickle', 'rb') as handle:\n",
    "        dico_poly_correction = pickle.load(handle)\n",
    "except:\n",
    "    dico_poly_correction =  get_dico_corrections(pairs)\n",
    "    with open('dico_poly_correction.pickle', 'wb') as handle:\n",
    "        pickle.dump(dico_poly_correction, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to correct observed tdoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_tdoa01(row, dico_poly_correction):\n",
    "    max_val = 2000\n",
    "    if (row.s0, row.s1) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s0, row.s1)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s0, row.s1)][2]):\n",
    "            if dico_poly_correction[(row.s0, row.s1)][1]<max_val:\n",
    "                return row.dt01-dico_poly_correction[(row.s0, row.s1)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s0, row.s1)][4]<max_val:\n",
    "            return row.dt01-dico_poly_correction[(row.s0, row.s1)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "def correct_tdoa02(row, dico_poly_correction):\n",
    "    max_val = 2000\n",
    "    if (row.s0, row.s2) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s0, row.s2)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s0, row.s2)][2]):\n",
    "            if dico_poly_correction[(row.s0, row.s2)][1]<max_val:\n",
    "                return row.dt02-dico_poly_correction[(row.s0, row.s2)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s0, row.s2)][4]<max_val:\n",
    "            return row.dt02-dico_poly_correction[(row.s0, row.s2)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "def correct_tdoa12(row, dico_poly_correction):\n",
    "    max_val = 2000\n",
    "    if (row.s1, row.s2) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s1, row.s2)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s1, row.s2)][2]):\n",
    "            if dico_poly_correction[(row.s1, row.s2)][1]<max_val:\n",
    "                return row.dt12-dico_poly_correction[(row.s1, row.s2)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s1, row.s2)][4]<max_val:\n",
    "            return row.dt12-dico_poly_correction[(row.s1, row.s2)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "\n",
    "def add_tdoa_confidence(row, dico_poly_correction):\n",
    "    score = 0\n",
    "    if (row.s0, row.s1) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s0, row.s1)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    if (row.s0, row.s2) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s0, row.s2)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    if (row.s1, row.s2) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s1, row.s2)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "def correct_tdoa01(row, dico_poly_correction):\n",
    "    max_val = 3000\n",
    "    if (row.s0, row.s1) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s0, row.s1)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s0, row.s1)][2]):\n",
    "            if dico_poly_correction[(row.s0, row.s1)][1]<max_val:\n",
    "                return row.dt01-dico_poly_correction[(row.s0, row.s1)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s0, row.s1)][4]<max_val:\n",
    "            return row.dt01-dico_poly_correction[(row.s0, row.s1)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "def correct_tdoa02(row, dico_poly_correction):\n",
    "    max_val = 3000\n",
    "    if (row.s0, row.s2) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s0, row.s2)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s0, row.s2)][2]):\n",
    "            if dico_poly_correction[(row.s0, row.s2)][1]<max_val:\n",
    "                return row.dt02-dico_poly_correction[(row.s0, row.s2)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s0, row.s2)][4]<max_val:\n",
    "            return row.dt02-dico_poly_correction[(row.s0, row.s2)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "def correct_tdoa12(row, dico_poly_correction):\n",
    "    max_val = 3000\n",
    "    if (row.s1, row.s2) in dico_poly_correction:\n",
    "        if np.min(dico_poly_correction[(row.s1, row.s2)][2]) < row.timeAtServer < np.max(dico_poly_correction[(row.s1, row.s2)][2]):\n",
    "            if dico_poly_correction[(row.s1, row.s2)][1]<max_val:\n",
    "                return row.dt12-dico_poly_correction[(row.s1, row.s2)][0](row.timeAtServer)\n",
    "        elif dico_poly_correction[(row.s1, row.s2)][4]<max_val:\n",
    "            return row.dt12-dico_poly_correction[(row.s1, row.s2)][3](row.timeAtServer)\n",
    "    return np.nan\n",
    "\n",
    "def add_tdoa_confidence(row, dico_poly_correction):\n",
    "    score = 0\n",
    "    if (row.s0, row.s1) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s0, row.s1)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    if (row.s0, row.s2) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s0, row.s2)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    if (row.s1, row.s2) in dico_poly_correction:\n",
    "        score += dico_poly_correction[(row.s1, row.s2)][1]\n",
    "    else:\n",
    "        return np.nan\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correction V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cor_v2_int(dico_poly_correction, id1, id2, thr, t, rdtxx):\n",
    "    offsets = []\n",
    "#     Case intermediary sensor in middle id1 < k[1] < id2\n",
    "    good_keys = [k  for k in dico_poly_correction if ((k[0]==id1) \n",
    "                 and (dico_poly_correction[k][2][0] <= t <= dico_poly_correction[k][2][-1])\n",
    "                 and ((k[1], id2) in dico_poly_correction) \n",
    "                 and  (dico_poly_correction[k][1]<thr)\n",
    "                 and  (dico_poly_correction[(k[1], id2)][1]<thr))]\n",
    "    for k in good_keys:\n",
    "        offset = rdtxx - (dico_poly_correction[k][0](t) + dico_poly_correction[k[1], id2][0](t))\n",
    "#         if np.abs(offset) < 1E7:\n",
    "        offsets.append(offset)\n",
    "    \n",
    "    # Case intermediary sensor right id1 < id2 < k[1]\n",
    "    good_keys = [k for k in dico_poly_correction if ((k[0]==id1) \n",
    "                 and (dico_poly_correction[k][2][0] <= t <= dico_poly_correction[k][2][-1])\n",
    "                 and ((id2, k[1]) in dico_poly_correction) \n",
    "                 and  (dico_poly_correction[k][1]<thr)\n",
    "                 and (dico_poly_correction[(id2, k[1])][1]<thr))]\n",
    "    for k in good_keys:\n",
    "        offset = rdtxx - (dico_poly_correction[k][0](t) - dico_poly_correction[id2, k[1]][0](t))\n",
    "#         if np.abs(offset) < 1E7:\n",
    "        offsets.append(offset)\n",
    "\n",
    "    # Case intermediary sensor left k[0] < id1 < id2\n",
    "#     good_keys = [k for k in dico_poly_correction if ((k[1]==id1) \n",
    "#                  and (dico_poly_correction[k][2][0] <= t <= dico_poly_correction[k][2][-1])\n",
    "#                  and ((k[0], id2) in dico_poly_correction) \n",
    "#                  and (dico_poly_correction[k][1]<thr)\n",
    "#                  and (dico_poly_correction[(k[0], id2)][1]<thr))]\n",
    "#     for k in good_keys:\n",
    "#         offset = rdtxx - (-dico_poly_correction[k][0](t) + dico_poly_correction[k[1], id2][0](t))\n",
    "#         if np.abs(offset) < 1E7:\n",
    "#             offsets.append(offset)\n",
    "\n",
    "    # Direct Pair case\n",
    "#     if (id1, id2) in dico_poly_correction:\n",
    "#         if (dico_poly_correction[(id1, id2)][2][0] <= t <= dico_poly_correction[(id1, id2)][2][-1]) \\\n",
    "#         and (dico_poly_correction[(id1, id2)][1] < thr):\n",
    "#             offset = rdtxx-dico_poly_correction[id1, id2][0](t)\n",
    "#             if np.abs(offset) < 1E7:\n",
    "#                 offsets.append(offset)\n",
    "            \n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_cor_v2_final(X_test_filtered_ac, dico_poly_correction):\n",
    "    thr = 1600\n",
    "    sol_01 = []\n",
    "    sol_02 = []\n",
    "    sol_12 = []\n",
    "    for _, r in tqdm(X_test_filtered_ac.iterrows(), total=len(X_test_filtered_ac)):\n",
    "        t = r.timeAtServer\n",
    "        id1, id2 = r.s0, r.s1\n",
    "        sol_01.append(get_cor_v2_int(dico_poly_correction, r.s0, r.s1, thr, t, r['dt01']))\n",
    "        sol_02.append(get_cor_v2_int(dico_poly_correction, r.s0, r.s2, thr, t, r['dt02']))\n",
    "        sol_12.append(get_cor_v2_int(dico_poly_correction, r.s1, r.s2, thr, t, r['dt12']))\n",
    "    X_test_filtered_ac['dt01_cor_v2'] = sol_01\n",
    "    X_test_filtered_ac['dt12_cor_v2'] = sol_12\n",
    "    X_test_filtered_ac['dt02_cor_v2'] = sol_02\n",
    "    X_test_filtered_ac['dt01_cor_v2_med'] = X_test_filtered_ac['dt01_cor_v2'].apply(lambda x: np.median(x))\n",
    "    X_test_filtered_ac['dt01_cor_v2_n'] = X_test_filtered_ac['dt01_cor_v2'].apply(lambda x: len(x))\n",
    "    X_test_filtered_ac['dt01_cor_v2_iqr'] = X_test_filtered_ac['dt01_cor_v2'].apply(lambda x: iqr(x))\n",
    "\n",
    "    X_test_filtered_ac['dt02_cor_v2_med'] = X_test_filtered_ac['dt02_cor_v2'].apply(lambda x: np.median(x))\n",
    "    X_test_filtered_ac['dt02_cor_v2_n'] = X_test_filtered_ac['dt02_cor_v2'].apply(lambda x: len(x))\n",
    "    X_test_filtered_ac['dt02_cor_v2_iqr'] = X_test_filtered_ac['dt02_cor_v2'].apply(lambda x: iqr(x))\n",
    "\n",
    "    X_test_filtered_ac['dt12_cor_v2_med'] = X_test_filtered_ac['dt12_cor_v2'].apply(lambda x: np.median(x))\n",
    "    X_test_filtered_ac['dt12_cor_v2_n'] = X_test_filtered_ac['dt12_cor_v2'].apply(lambda x: len(x))\n",
    "    X_test_filtered_ac['dt12_cor_v2_iqr'] = X_test_filtered_ac['dt12_cor_v2'].apply(lambda x: iqr(x))\n",
    "\n",
    "    # Here we try to give a score the lower the better\n",
    "#     X_test_filtered_ac['score'] = (X_test_filtered_ac['dt01_cor_v2_iqr']/(0.5*X_test_filtered_ac['dt01_cor_v2_n']) +\n",
    "#         X_test_filtered_ac['dt02_cor_v2_iqr']/(0.5*X_test_filtered_ac['dt02_cor_v2_n']) +\n",
    "#         X_test_filtered_ac['dt12_cor_v2_iqr']/(0.5*X_test_filtered_ac['dt12_cor_v2_n']))\n",
    "    \n",
    "    X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt01_cor)) & (X_test_filtered_ac.dt01_cor_v2_iqr<1000) & (X_test_filtered_ac.dt01_cor_v2_n>3), 'dt01_cor'] = X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt01_cor)) & (X_test_filtered_ac.dt01_cor_v2_iqr<1000) & (X_test_filtered_ac.dt01_cor_v2_n>3), 'dt01_cor_v2_med']\n",
    "    X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt02_cor)) & (X_test_filtered_ac.dt02_cor_v2_iqr<1000) & (X_test_filtered_ac.dt02_cor_v2_n>3), 'dt02_cor'] = X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt02_cor)) & (X_test_filtered_ac.dt02_cor_v2_iqr<1000) & (X_test_filtered_ac.dt02_cor_v2_n>3), 'dt02_cor_v2_med']\n",
    "    X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt12_cor)) & (X_test_filtered_ac.dt12_cor_v2_iqr<1000) & (X_test_filtered_ac.dt12_cor_v2_n>3), 'dt12_cor'] = X_test_filtered_ac.loc[(np.isnan(X_test_filtered_ac.dt12_cor)) & (X_test_filtered_ac.dt12_cor_v2_iqr<1000) & (X_test_filtered_ac.dt12_cor_v2_n>3), 'dt12_cor_v2_med']\n",
    "    return X_test_filtered_ac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilateration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "\n",
    "def MLAT3(p, *data):\n",
    "    eqs = []\n",
    "    dico_eq = {}\n",
    "    for _, group in data[0].iterrows():\n",
    "        id0, id1, id2 = group.s0, group.s1, group.s2\n",
    "        c = np.float64(0.2995) # Transmission speed\n",
    "        x, y = p\n",
    "        dt01 = group.dt01_cor\n",
    "        dt02 = group.dt02_cor\n",
    "        dt12 = group.dt12_cor\n",
    "        s0 = tuple(group[['s0_lat', 's0_lon']].values)\n",
    "        s1 = tuple(group[['s1_lat', 's1_lon']].values)\n",
    "        s2 = tuple(group[['s2_lat', 's2_lon']].values)\n",
    "        try:\n",
    "            ap_alt = np.float64(group.geoAltitude)\n",
    "        except:\n",
    "            ap_alt = np.float64(group.baroAltitude)\n",
    "        vert0 = ap_alt-group.s0_alt\n",
    "        vert1 = ap_alt-group.s1_alt\n",
    "        vert2 = ap_alt-group.s2_alt\n",
    "        \n",
    "        if ((id0, id1)) not in dico_eq:\n",
    "            dico_eq[(id0, id1)] = 0\n",
    "            eqs.append(\n",
    "                (np.sqrt(geopy.distance.distance(s0, (x, y)).m**2 + vert0**2)\n",
    "                 -np.sqrt(geopy.distance.distance(s1, (x, y)).m**2 + vert1**2))/c-dt01,)\n",
    "        if ((id0, id2)) not in dico_eq:\n",
    "            dico_eq[(id0, id2)] = 0\n",
    "            eqs.append(\n",
    "                (np.sqrt(geopy.distance.distance(s0, (x, y)).m**2 + vert0**2)\n",
    "                -np.sqrt(geopy.distance.distance(s2, (x, y)).m**2 + vert2**2))/c-dt02,)        \n",
    "        if ((id1, id2)) not in dico_eq:\n",
    "            dico_eq[(id1, id2)] = 0\n",
    "            eqs.append(\n",
    "                (np.sqrt(geopy.distance.distance(s1, (x, y)).m**2 + vert1**2)\n",
    "                 -np.sqrt(geopy.distance.distance(s2, (x, y)).m**2 + vert2**2))/c-dt12,) \n",
    "    return eqs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dico_res_lon = {}\n",
    "dico_res_lat = {}\n",
    "list_dico_sol = []\n",
    "\n",
    "\n",
    "def get_sol_mlat_all_in_view(list_ac_ids):\n",
    "    sol_path = 'pickled_sol'\n",
    "    df_sol = pd.DataFrame()\n",
    "    for ac_id in list_ac_ids:\n",
    "        if os.path.isfile(sol_path+'/df_sol_mlat_{}.pkl'.format(ac_id)):\n",
    "            continue\n",
    "        list_dico_sol = []\n",
    "        X_test_filtered_ac = X_test_filtered.loc[\n",
    "            X_test_filtered.id.isin(df_testing.loc[df_testing.aircraft==ac_id].id.unique())]\n",
    "        X_test_filtered_ac.geoAltitude = X_test_filtered_ac.baroAltitude+np.median(X_test_filtered_ac.geoAltitude-X_test_filtered_ac.baroAltitude)\n",
    "\n",
    "        if len(X_test_filtered_ac)==0:\n",
    "            df_sol = pd.DataFrame()\n",
    "            df_sol.to_pickle(sol_path+'/df_sol_mlat_{}.pkl'.format(ac_id))\n",
    "            continue\n",
    "            \n",
    "        # correct dt_obs\n",
    "        if True:\n",
    "            X_test_filtered_ac['dt01_cor'] = X_test_filtered_ac.apply(lambda r: correct_tdoa01(r, dico_poly_correction), 1)\n",
    "            X_test_filtered_ac['dt02_cor'] = X_test_filtered_ac.apply(lambda r: correct_tdoa02(r, dico_poly_correction), 1)\n",
    "            X_test_filtered_ac['dt12_cor'] = X_test_filtered_ac.apply(lambda r: correct_tdoa12(r, dico_poly_correction), 1)\n",
    "            X_test_filtered_ac = get_cor_v2_final(X_test_filtered_ac, dico_poly_correction)\n",
    "            X_test_filtered_ac = X_test_filtered_ac.dropna(subset=['dt01_cor', 'dt02_cor', 'dt12_cor'])\n",
    "            X_test_filtered_ac['tdoa_conf'] = X_test_filtered_ac.apply(lambda r: add_tdoa_confidence(r, dico_poly_correction), 1)\n",
    "        \n",
    "        else:\n",
    "            df_sol.to_pickle(sol_path+'/df_sol_mlat_{}.pkl'.format(ac_id))\n",
    "            continue\n",
    "        for ids, group in tqdm(X_test_filtered_ac.groupby('id'), total=len(X_test_filtered_ac.id.unique())):\n",
    "            if True:\n",
    "                centroid = np.median(group[['s0_lat', 's1_lat', 's2_lat']].values.flatten()), np.median(group[['s0_lon', 's1_lon', 's2_lon']].values.flatten())\n",
    "                group = group.loc[(np.abs(group.dt01_cor) < 1e7) &\n",
    "                                  (np.abs(group.dt02_cor) < 1e7) &\n",
    "                                  (np.abs(group.dt12_cor) < 1e7)]\n",
    "                group = group.loc[group.tdoa_conf<9000]\n",
    "#                 group = group.sort_values('score')\n",
    "                group = group.sort_values('tdoa_conf')\n",
    "\n",
    "                group = group.iloc[:10]\n",
    "                sample = group.iloc[0]\n",
    "                guess = centroid\n",
    "                \n",
    "                # Bad first guess based on visual inspection\n",
    "                if ac_id == 425:\n",
    "                    guess = 40.05, -6.75\n",
    "                if ac_id == 1515:\n",
    "                    guess = 44.88, 15.11\n",
    "\n",
    "                if len(list_dico_sol) > 0:\n",
    "                    guess = list_dico_sol[-1]['lat'], list_dico_sol[-1]['lon']\n",
    "                \n",
    "                solution = root(MLAT3, guess, args=group, method='lm')\n",
    "                if solution.status not in [1, 2, 3]:\n",
    "                    continue\n",
    "                list_dico_sol.append({'ID': ids,\n",
    "                      'timeAtServer': sample.timeAtServer,\n",
    "                      'guess': guess,\n",
    "                      'lat': solution.x[0],\n",
    "                      'lon': (solution.x[1] % 360 + 540) % 360 - 180,\n",
    "                      'centroid_lat': centroid[0],\n",
    "                      'centroid_lon': centroid[1],\n",
    "                      'n':len(group),\n",
    "                      'mean_fval': np.mean(solution.fun),\n",
    "                      'qtf': solution.qtf,\n",
    "                      'iqr_fval': iqr(solution.fun)\n",
    "                     }\n",
    "                    )\n",
    "            else:\n",
    "                pass\n",
    "        df_sol = pd.DataFrame(list_dico_sol)\n",
    "        df_sol.to_pickle(sol_path+'/df_sol_mlat_{}.pkl'.format(ac_id))\n",
    "\n",
    "def get_sol_mlat_all_in_view_para(X_test_filtered2, df_testing2, sensors2):\n",
    "    # Here we use multoprocessing to make things faster\n",
    "    global X_test_filtered\n",
    "    global df_testing\n",
    "    global sensors\n",
    "\n",
    "    X_test_filtered = X_test_filtered2.copy()\n",
    "    df_testing = df_testing2.copy()\n",
    "    sensors = sensors2.copy()\n",
    "    \n",
    "    dir_path = 'pickled_sol'\n",
    "    onlyfiles = [f for f in listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "    str_pkl_files = ''.join(onlyfiles)\n",
    "    num_processes = mp.cpu_count()\n",
    "    temp = re.findall(r'\\d+', str_pkl_files) \n",
    "    ids_done = list(map(int, temp)) \n",
    "    \n",
    "    list_ids = list(set(list(df_testing.aircraft.unique()))-set(ids_done))\n",
    "    print('IDs to be computed:', len(list_ids))\n",
    "    if len(list_ids)==0:\n",
    "        return\n",
    "    chunk_size = int((len(list_ids)//num_processes)+1)                      \n",
    "    chunks = [list_ids[i:i + chunk_size]for i in range(0, len(list_ids), chunk_size)]\n",
    "    print('Start // computation')\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        list_df = pool.map(get_sol_mlat_all_in_view, chunks)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    return pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filtered = X_test.copy()\n",
    "get_sol_mlat_all_in_view_para(X_test, df_testing, sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "path_pkl = 'pickled_sol'\n",
    "\n",
    "df_sols = []\n",
    "df_sols3 = []\n",
    "onlyfiles = [f for f in listdir(path_pkl) if isfile(join(path_pkl, f))]\n",
    "\n",
    "def is_outlier(points, thresh=3):\n",
    "    if len(points.shape) == 1:\n",
    "        points = points[:,None]\n",
    "    median = np.median(points, axis=0)\n",
    "    diff = np.sum((points - median)**2, axis=-1)\n",
    "    diff = np.sqrt(diff)\n",
    "    med_abs_deviation = np.median(diff)\n",
    "\n",
    "    modified_z_score = 0.6745 * diff / med_abs_deviation\n",
    "\n",
    "    return modified_z_score > thresh\n",
    "\n",
    "def filter_results(selection, var, aircraft, n_poly=3, delta_t=20):\n",
    "    tas = testing[testing.aircraft==aircraft]\n",
    "    selection['gt'] = selection.timeAtServer.apply(lambda x: (x-selection.timeAtServer.values[0])//delta_t).astype(int)\n",
    "    x = selection.timeAtServer.values\n",
    "    y = selection[var].values\n",
    "    dico_tp = {}\n",
    "\n",
    "    gtimes = {}\n",
    "    for gt, group in selection.groupby('gt'):\n",
    "        n_poly = np.min([len(group)//5,  n_poly])+1\n",
    "        dico_tp[gt] = np.poly1d(np.polyfit(group.timeAtServer, group[var], n_poly))\n",
    "        gtimes[gt] = group.timeAtServer.values.tolist()\n",
    "    var2 = []\n",
    "    for gt in dico_tp:\n",
    "        var2 += dico_tp[gt](gtimes[gt]).tolist()\n",
    "    selection['var2'] = var2\n",
    "    selection['var_err'] = (selection.var2-selection[var]).abs()\n",
    "\n",
    "\n",
    "    selection = selection.loc[~is_outlier(selection['var_err'], 3.5)]\n",
    "\n",
    "    x = selection.timeAtServer.values\n",
    "    y = selection[var].values\n",
    "    gtimes = {}\n",
    "    dico_tp = {}\n",
    "\n",
    "    for gt, group in selection.groupby('gt'):\n",
    "        n_poly = np.min([len(group)//5,  n_poly])+1\n",
    "        dico_tp[gt] = np.poly1d(np.polyfit(group.timeAtServer, group[var], n_poly))\n",
    "        gtimes[gt] = group.timeAtServer.values.tolist()\n",
    "    var2 = []\n",
    "    new_times = []\n",
    "    ids = []\n",
    "    for gt in dico_tp:\n",
    "        if len(gtimes[gt])<2:\n",
    "            continue\n",
    "        inter_df = get_time_inter(gtimes[gt], tas)\n",
    "        t_inter = inter_df.timeAtServer.values.tolist()\n",
    "        var2 += dico_tp[gt](t_inter).tolist()\n",
    "        ids += inter_df.id.values.tolist()\n",
    "        new_times += t_inter\n",
    "    df = pd.DataFrame()\n",
    "    df['timeAtServer'] = new_times\n",
    "    df[var] = var2\n",
    "    df['id'] = ids\n",
    "    return df\n",
    "\n",
    "def get_time_inter(gt, tas):\n",
    "    return tas[(gt[0]<=tas.timeAtServer) & (tas.timeAtServer<=gt[-1])]\n",
    "\n",
    "def process_results(onlyfiles, n_min=1, qtf_max=1, delta_t=30, n_poly=3, plt_en=True):\n",
    "    for cpt, f in enumerate(onlyfiles[:]):\n",
    "        temp = re.findall(r'\\d+', f) \n",
    "        ids = list(map(int, temp))[0]\n",
    "\n",
    "        path_pkl = 'pickled_sol'\n",
    "        \n",
    "        tac = testing.loc[testing.aircraft==ids]\n",
    "        mlat = pd.read_pickle(join(path_pkl, 'df_sol_mlat_{}.pkl'.format(ids) ))\n",
    "        if len(mlat) > 0 :\n",
    "            if len(set(mlat.ID) & set(tac.id)) < len(set(mlat.ID)):\n",
    "                continue\n",
    "#                 print('Achtung', ids, len(mlat), len(set(mlat.ID) & set(tac.id)))\n",
    "                \n",
    "        if ids in [357, 1570, 138, 1913, 2820, 1832, 2910, 1677, 847, 994, 1228, 1569, 1568, 182, 1887, 2144, 2459, 2744]:\n",
    "            continue\n",
    "        if plt_en:\n",
    "            \n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(19, 3))\n",
    "            try:\n",
    "                lb = last_best.loc[last_best.id.isin(testing.loc[testing.aircraft==ids].id)]\n",
    "                ax1.scatter(x=lb['timeAtServer'], y=lb['latitude'], c='orange')\n",
    "                ax2.scatter(x=lb['timeAtServer'], y=lb['longitude'], c='orange')\n",
    "                ax3.scatter(x=lb['longitude'], y=lb['latitude'], c='orange')\n",
    "                plt.title(str(ids))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        df_sol = pd.read_pickle(path_pkl+'/'+'df_sol_mlat_{}.pkl'.format(ids))\n",
    "        if len(df_sol)<1:\n",
    "            continue\n",
    "        df_sol.qtf = df_sol.qtf.apply(lambda x: np.abs(x[0])+np.abs(x[1]))\n",
    "\n",
    "        selection = df_sol.loc[(df_sol.n >= n_min) & \n",
    "                               (df_sol.qtf <= qtf_max) & \n",
    "                               (np.abs(df_sol.lat-df_sol.centroid_lat)<3) &\n",
    "                               (np.abs(df_sol.lon-df_sol.centroid_lon)<3)\n",
    "                              ].reset_index()\n",
    "\n",
    "        if plt_en:\n",
    "#             pass\n",
    "            ax1.scatter(x=selection['timeAtServer'], y=selection['lat'], c='blue', s=10)\n",
    "            ax2.scatter(x=selection['timeAtServer'], y=selection['lon'], c='blue', s=10)\n",
    "#             ax3.scatter(x=selection['lon'], y=selection['lat'], c='blue', s=10)\n",
    "\n",
    "        samp = sample.loc[sample.id.isin(testing[testing.aircraft==ids].id)]\n",
    "    #     break\n",
    "        samp = samp.set_index('id').join(selection[['lat', 'lon', 'ID']].set_index('ID'), rsuffix='_')\n",
    "        samp.latitude.fillna(samp.lat, inplace=True)\n",
    "        samp.longitude.fillna(samp.lon, inplace=True)\n",
    "        samp = samp.dropna(subset=['latitude'])\n",
    "        samp = samp.dropna(subset=['longitude'])\n",
    "        selection = samp.drop(['lat', 'lon'], axis=1)\n",
    "        selection = selection.rename(columns={'latitude':'lat', 'longitude':'lon'})\n",
    "\n",
    "        if plt_en:\n",
    "            ax1.scatter(x=selection['timeAtServer'], y=selection['lat'], c='black', s=10)\n",
    "            ax2.scatter(x=selection['timeAtServer'], y=selection['lon'], c='black', s=10)\n",
    "#             ax3.scatter(x=selection['lon'], y=selection['lat'], c='blue', s=10)\n",
    "            ax2.set_title(str(ids))\n",
    "\n",
    "        filtered_selection_lon = filter_results(selection.dropna(subset=['lon']), 'lon', ids, n_poly=n_poly, delta_t=delta_t)\n",
    "        filtered_selection_lat = filter_results(selection.dropna(subset=['lat']), 'lat', ids, n_poly=n_poly, delta_t=delta_t)\n",
    "        sample.loc[sample.id.isin(filtered_selection_lon.id.values), 'longitude'] = filtered_selection_lon.lon.values\n",
    "        sample.loc[sample.id.isin(filtered_selection_lat.id.values), 'latitude'] = filtered_selection_lat.lat.values\n",
    "\n",
    "        samp = sample.loc[sample.id.isin(testing[testing.aircraft==ids].id)]\n",
    "#         samp_ac = sample.loc[sample.id.isin(filtered_selection_lon.id.values)]\n",
    "        if plt_en:\n",
    "            print(ids)\n",
    "            ax1.scatter(x=samp['timeAtServer'], y=samp['latitude'], c='red', s=1, zorder=99999)\n",
    "            ax2.scatter(x=samp['timeAtServer'], y=samp['longitude'], c='red', s=1, zorder=99999)\n",
    "            ax3.scatter(x=samp['longitude'], y=samp['latitude'], c='red', s=1)\n",
    "            print(100*(len(samp.dropna(subset=['latitude']).dropna(subset=['longitude']))/len(sample)))\n",
    "            ax1.set_title(str(ids))\n",
    "            plt.show()\n",
    "    return sample\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = training.loc[training.latitude.isnull()]\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('round2_sample_empty.csv')\n",
    "print(len(sample))\n",
    "sample = sample.set_index('id').join(testing.set_index('id')['timeAtServer']).reset_index()\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEst one\n",
    "n = 300\n",
    "sample = process_results(onlyfiles[:n], n_min=3, qtf_max=1e-3, delta_t=30, n_poly=3, plt_en=False)\n",
    "sample = process_results(onlyfiles[:n], n_min=3, qtf_max=1e-3, delta_t=70, n_poly=3, plt_en=False)\n",
    "\n",
    "print('done')\n",
    "sample = process_results(onlyfiles[:n], n_min=2, qtf_max=1e-1, delta_t=70, n_poly=4, plt_en=False)\n",
    "sample = process_results(onlyfiles[:n], n_min=1, qtf_max=1, delta_t=100, n_poly=4, plt_en=False)\n",
    "sample = process_results(onlyfiles[:n], n_min=1, qtf_max=2, delta_t=200, n_poly=4, plt_en=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc = 100*(len(sample.dropna(subset=['latitude']).dropna(subset=['longitude']))/len(sample))\n",
    "print(perc)\n",
    "if perc >= 70:\n",
    "    sample = sample[['id', 'latitude','longitude','geoAltitude']]\n",
    "    sample.to_csv('sol_osn_comp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-osn]",
   "language": "python",
   "name": "conda-env-.conda-osn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
